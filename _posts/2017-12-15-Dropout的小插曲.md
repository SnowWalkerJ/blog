---
layout: post
title: Dropout的小插曲
date: 2017-12-15 15:58:27
tags:
 - 深度学习
layout: post
---

在Dropout的论文中，在evaluation过程中，由于不执行dropout，为了防止scale变大，要把权重乘以p（权重不被drop的概率），这样就可以把输出值当成所有带dropout的输出的平均值，相当于做了ensemble。

<!-- more -->

Training:

$$mask \sim Bernoulli(p)$$

$$\tilde{x} = x * mask$$

$$\tilde{y} = \tilde{x}W^T + b$$

Evaluation:
$$
    y = xW^T*p + b \\
$$

## 问题
简化一下模型，只考虑单层，有N个输入，1个输出，$$ y=\sum_{i=1}^{N}W_ix_i $$（不考虑偏置），假如$$W_ix_i$$之间是$$i.i.d.$$的，那么y的方差应该等于各项方差的和。

Training：

$$Var(\tilde{y})=\sum_{i=1}^NMask_iVar(W_ix_i) = pN\sigma^2 $$

$$Std(\tilde{y})=\sqrt{Np}\sigma$$

Evalutation:

$$Var(y)=\sum_{i=1}^NpVar(W_ix_i) = p^2N\sigma^2 $$

$$Std(y)=p\sqrt{N}\sigma$$

那么问题就来了，训练过程和测试过程中Dropout层输出的scale是不一致的，这肯定会导致问题！正确的做法难道不应该是在测试过程中让权重乘以$$\sqrt{p}$$而不是$$p$$嘛！
为什么这个问题一直没有被大家发现呢！

## 解决
仔细思考一下，其实$$i.i.d.$$的假设一开始就是不成立的。在模型得到充分训练的前提下，不同的mask采样应该会对应相似的输出（这也是dropout的初衷，避免对个别输入过于敏感），那么此时$$W_ix_i$$是高度相关的，对应的标准差应该接近$$Np\sigma$$（训练期），那么测试期用$$p$$作为标准化系数去乘以权重就可以得到一致的结果。

反过来想，如果不同mask采样对应的输出值相差很远呢？那么就意味着这个输出的不确定性是非常大的，那么在测试期乘以$$p$$确实会缩小scale，这种scale的缩小可以有效减小不确定性带来的误差，也是合理的。

（数学公式上有点偷懒了）
